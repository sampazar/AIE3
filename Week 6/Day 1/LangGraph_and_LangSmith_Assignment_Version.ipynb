{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks for the description of the tool and then uses that to decide what tool, if any, is best for the particular task or request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle? No.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles? We could specify a maximum number of loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of Large Language Models (LLMs) to enhance their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to improve the quality and relevance of generated text by incorporating external information retrieved from a large corpus of documents.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates the final response by conditioning on both the input query and the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Document Retrieval**: The retriever searches a large corpus to find documents or passages that are relevant to the query.\n",
            "3. **Response Generation**: The generator uses the retrieved documents along with the original query to generate a coherent and contextually relevant response.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Enhanced Knowledge**: By leveraging external documents, RAG can provide more accurate and up-to-date information.\n",
            "- **Contextual Relevance**: The retrieval step ensures that the generated text is more contextually relevant to the query.\n",
            "- **Scalability**: It can handle a vast amount of information by retrieving only the most relevant pieces.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated that RAG could significantly improve the performance of various knowledge-intensive tasks, such as open-domain question answering and fact-checking, by combining retrieval and generation in a unified framework.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-06-12\n",
            "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "Summary: There are various methods for adapting LLMs to different domains. The most\n",
            "common methods are prompting, finetuning, and RAG. In this w\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: ### What is QLoRA in Machine Learning?\n",
            "\n",
            "QLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\n",
            "\n",
            "1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\n",
            "2. **Performance Preservation**: Maintains full 16-bit finetuning task performance.\n",
            "3. **Innovations**:\n",
            "   - **4-bit NormalFloat (NF4)**: A new data type optimal for normally distributed weights.\n",
            "   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
            "   - **Paged Optimizers**: Manages memory spikes during training.\n",
            "\n",
            "The approach backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family developed using QLoRA, named Guanaco, outperforms previous models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance with only 24 hours of finetuning on a single GPU.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Summary**: This paper introduces QLoRA and its innovations, providing a detailed analysis of instruction following and chatbot performance across various datasets and model scales.\n",
            "\n",
            "2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention, achieving significant performance gains with minimal additional time consumption.\n",
            "\n",
            "3. **Title**: [Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "   - **Summary**: This paper explores various methods for adapting LLMs to different domains, including QLoRA, and assesses their quality and performance.\n",
            "\n",
            "### Bio of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a researcher focused on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. His work involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "-Received the query about QLoRA and relevant technical papers, followed by a request for the bio of the first author.\n",
        "-Queried Arxiv with \"QLoRA\" to find relevant technical papers.\n",
        "-Found and summarized three relevant papers on QLoRA.\n",
        "-Searched for the bio of Tim Dettmers using DuckDuckGo.\n",
        "-Found and summarized the bio of Tim Dettmers.\n",
        "-Provided an explanation of QLoRA, summarized the technical papers, and included the bio of Tim Dettmers in the final response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Hereâ€™s a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This can involve concatenating the retrieved information with the original query or using it to provide additional context.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers by leveraging the additional information provided by the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model's parameters but can be found in external documents. This approach helps in generating more accurate and informative responses, especially for complex queries that require specific knowledge.\""
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is the main benefit of using QLoRA?\",\n",
        "    \"Which institutions were involved in the QLoRA research?\",\n",
        "    \"What type of tasks can QLoRA be applied to?\",\n",
        "    \"How does QLoRA improve computational efficiency?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"efficient\", \"training\"]},\n",
        "    {\"must_mention\" : [\"institutions\", \"research\"]},\n",
        "    {\"must_mention\" : [\"NLP\", \"tasks\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions? They are associated by element, which could potentially cause issues with ordering.\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is? Challenges include a mismatched case (lower case vs upper case) and different words with similar meanings. One way to address this could be going from a binary to a 0 to 1 scale for the score or another method to give partial credit.\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 8a00dbbb' at:\n",
            "https://smith.langchain.com/o/b67ec661-41d9-5c3a-912e-db0cfaa37e4e/datasets/3c27ccb1-a3d3-4c52-a7a8-23b50a2e5f91/compare?selectedSessions=0582a4f3-cca4-4616-8d7c-f02b3767253a\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 8501b7ff at:\n",
            "https://smith.langchain.com/o/b67ec661-41d9-5c3a-912e-db0cfaa37e4e/datasets/3c27ccb1-a3d3-4c52-a7a8-23b50a2e5f91\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>07cf1e1e-d1e6-4d24-8b2f-58ea02b2c7b7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.343205</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.430297</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.548536</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.349994</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.233817</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.453938</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.127028</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                               5.0   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                               1.0   \n",
              "std                      0.0                               0.0   \n",
              "min                      1.0                               1.0   \n",
              "25%                      1.0                               1.0   \n",
              "50%                      1.0                               1.0   \n",
              "75%                      1.0                               1.0   \n",
              "max                      1.0                               1.0   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        5.343205   \n",
              "std                      NaN   NaN        1.430297   \n",
              "min                      NaN   NaN        3.548536   \n",
              "25%                      NaN   NaN        4.349994   \n",
              "50%                      NaN   NaN        5.233817   \n",
              "75%                      NaN   NaN        6.453938   \n",
              "max                      NaN   NaN        7.127028   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     07cf1e1e-d1e6-4d24-8b2f-58ea02b2c7b7  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 8a00dbbb',\n",
              " 'results': {'0c12fc75-9d16-4be9-85af-903fa1c4389a': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of the optimizer used in QLoRA, which is the \"paged optimizer\". It explains how this optimizer works and why it is used, which is to manage memory spikes and allow for efficient memory management. This information is directly related to the input question and provides a clear and comprehensive answer. \\n\\nThe submission also provides additional context about QLoRA, explaining that it is used for the fine-tuning of large language models and that it is designed to work on hardware with limited memory capacity. This information is not directly asked for in the input question, but it helps to provide a fuller understanding of the topic and is therefore helpful.\\n\\nIn conclusion, the submission is helpful, insightful, and appropriate. It provides a clear and comprehensive answer to the input question and offers additional context that enhances understanding of the topic.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('38a7c4ca-6c46-44a0-8093-c391da24c434'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies that QLoRA uses paged optimizers. The student also provides additional context about why paged optimizers are used, which is not in conflict with the context provided. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('99f4dae1-22fa-4f66-b8df-8af8eb888c43'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ab9e3374-8317-49aa-a8ca-d55b3d0b77e5'), target_run_id=None)],\n",
              "   'execution_time': 5.137759,\n",
              "   'run_id': '07cf1e1e-d1e6-4d24-8b2f-58ea02b2c7b7',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of techniques to optimize memory usage and performance during the fine-tuning of large language models. One of the key innovations mentioned in the QLoRA paper is the use of \"paged optimizers\" to manage memory spikes. This approach allows for efficient memory management, enabling the fine-tuning of large models on hardware with limited memory capacity.\\n\\nIn summary, QLoRA employs paged optimizers as part of its strategy to efficiently fine-tune quantized large language models.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '28a02861-4a39-4774-8b9c-1e2bae426bf5': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating that the QLoRA paper introduced a new data type called 4-bit NormalFloat (NF4). \\n\\nIn addition to providing the direct answer, the submission also provides additional information about the purpose and function of the new data type, explaining that it is designed to be optimal for normally distributed weights and helps in reducing memory usage while preserving performance during the finetuning of large language models. \\n\\nThis additional information is relevant and insightful, as it provides context and understanding about why the new data type was created and what it is used for. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3c76c17f-859b-46fc-b02c-e95dda8cb782'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer matches the context given. The context mentions 'NF4' and 'NormalFloat', which the student correctly identifies as the new data type introduced in the QLoRA paper. The student also provides additional information about the purpose of this data type, which does not conflict with the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a2380c9c-e3d8-432c-9c6e-a391c5abb5c4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('be7af46f-aa7d-4584-8570-f874042d1f1c'), target_run_id=None)],\n",
              "   'execution_time': 4.087406,\n",
              "   'run_id': '615caee3-9e68-420d-a6cd-96eb98e0d2bd',\n",
              "   'output': 'In the QLoRA paper, a new data type called **4-bit NormalFloat (NF4)** was introduced. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '905f609d-2f80-4a62-927f-4b6104ab7f49': {'input': {'question': 'What is the main benefit of using QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of QLoRA and its benefits. It not only defines what QLoRA is but also explains how it works and why it is beneficial. The submission is insightful as it provides a comprehensive understanding of the topic, including aspects like computational resources, efficiency, model performance, scalability, and cost-effectiveness. \\n\\nThe submission is appropriate as it directly answers the question asked in the input. It focuses on the main benefit of using QLoRA and provides additional information that enhances the understanding of the topic. \\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f19cc976-dc5a-40f2-81d7-ec02633cd306'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and covers the main benefit of using QLoRA, which is efficiency. The student explains how QLoRA reduces computational resources, allows for efficient fine-tuning, preserves model performance, scales the fine-tuning process, and is cost-effective. All these points contribute to the overall efficiency of the process. The student's answer aligns with the context provided, which is 'efficient' and 'training'. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d2bddfb4-412b-4441-9fbb-a26d4e397fff'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0f73c572-2145-4ab2-ab9a-de5e1f002c11'), target_run_id=None)],\n",
              "   'execution_time': 7.127028,\n",
              "   'run_id': '3fbbc992-5151-4535-9452-0a428caf1cce',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) is a technique used in machine learning, particularly in the context of fine-tuning large language models. The main benefits of using QLoRA include:\\n\\n1. **Reduced Computational Resources**: QLoRA significantly reduces the computational resources required for fine-tuning large models. By quantizing the model weights and using low-rank adaptation, it minimizes the memory footprint and computational overhead.\\n\\n2. **Efficiency**: It allows for efficient fine-tuning by focusing on a smaller, more manageable subset of parameters. This makes it feasible to fine-tune large models on smaller hardware setups, such as consumer-grade GPUs.\\n\\n3. **Preservation of Model Performance**: Despite the reduction in computational resources, QLoRA aims to preserve the performance of the original model. The low-rank adaptation helps in maintaining the expressiveness and accuracy of the model while being more resource-efficient.\\n\\n4. **Scalability**: QLoRA makes it easier to scale the fine-tuning process to larger models and datasets. This is particularly useful in research and industry settings where large-scale models are common.\\n\\n5. **Cost-Effectiveness**: By reducing the need for high-end hardware and extensive computational resources, QLoRA can lower the cost associated with fine-tuning large language models.\\n\\nOverall, QLoRA provides a practical and efficient approach to fine-tuning large language models, making advanced machine learning techniques more accessible and cost-effective.',\n",
              "   'reference': {'must_mention': ['efficient', 'training']}},\n",
              "  '8aaddb3e-6de1-403f-86d2-d47d7a4223a8': {'input': {'question': 'Which institutions were involved in the QLoRA research?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides an answer to the question asked in the input, which is about the institutions involved in the QLoRA research. The submission states that the University of Washington\\'s UW NLP group was primarily involved in the development of the QLoRA research. \\n\\nThis information is helpful as it directly answers the question. It is insightful as it provides specific details about the institution involved. It is also appropriate as it is relevant to the question asked. \\n\\nTherefore, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('abbc6744-a18d-4e42-a19e-20d6782e3b3f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context does not provide any information about which institutions were involved in the QLoRA research. Therefore, there is no way to verify the accuracy of the student's answer.\\nGRADE: Cannot be determined.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a5e101a1-c08e-4818-981e-514335eba6bf'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7b722123-c0ef-4a25-b59a-a10339289075'), target_run_id=None)],\n",
              "   'execution_time': 3.548536,\n",
              "   'run_id': '603e22eb-754b-4525-8b86-39113cbd5461',\n",
              "   'output': \"The QLoRA research was primarily developed by members of the University of Washington's UW NLP group.\",\n",
              "   'reference': {'must_mention': ['institutions', 'research']}},\n",
              "  'b0a29a20-e3cc-416b-8454-c6e3c92a0f14': {'input': {'question': 'What type of tasks can QLoRA be applied to?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of what QLoRA is and the types of tasks it can be applied to. It lists ten different tasks, providing a brief explanation of each one. This gives the reader a comprehensive understanding of the versatility and applicability of QLoRA. \\n\\nFurthermore, the submission also explains the primary advantage of QLoRA, which adds to its helpfulness by providing context on why one might choose to use this technique. \\n\\nBased on this analysis, the submission is helpful, insightful, and appropriate. It provides a thorough answer to the question and offers additional context that enhances its value.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('406b475f-27a0-4098-a675-308b5026e82a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context provided is 'NLP', 'tasks'. The student's answer lists a variety of tasks that QLoRA can be applied to, all of which fall under the umbrella of Natural Language Processing (NLP) tasks. The student also provides a brief explanation of what QLoRA is and its advantages, which aligns with the context. Therefore, the student's answer is factually accurate and relevant to the context provided.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7a914d58-105c-4390-9c6b-d2695e606b57'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1b9af004-7557-47db-8520-9e054c56b7e3'), target_run_id=None)],\n",
              "   'execution_time': 6.828625,\n",
              "   'run_id': '4bfba1b3-916d-4315-8ed7-572015b64596',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) is a technique used to fine-tune large language models efficiently. It can be applied to a variety of tasks, including but not limited to:\\n\\n1. **Text Classification**: Assigning categories or labels to text based on its content.\\n2. **Sentiment Analysis**: Determining the sentiment expressed in a piece of text (e.g., positive, negative, neutral).\\n3. **Named Entity Recognition (NER)**: Identifying and classifying entities (e.g., names of people, organizations, locations) within text.\\n4. **Question Answering**: Providing answers to questions based on a given context or dataset.\\n5. **Text Generation**: Generating coherent and contextually relevant text based on a given prompt.\\n6. **Machine Translation**: Translating text from one language to another.\\n7. **Summarization**: Producing concise summaries of longer texts.\\n8. **Dialogue Systems**: Creating conversational agents or chatbots that can interact with users in a natural language.\\n9. **Text-to-SQL**: Converting natural language queries into SQL queries.\\n10. **Code Generation**: Generating code snippets or entire programs based on natural language descriptions.\\n\\nThe primary advantage of QLoRA is that it allows for the fine-tuning of large models with reduced computational resources by leveraging quantization and low-rank adaptation techniques. This makes it particularly useful for scenarios where computational efficiency and resource constraints are important considerations.',\n",
              "   'reference': {'must_mention': ['NLP', 'tasks']}},\n",
              "  '50dad621-932b-4df3-b4a9-a893331692cd': {'input': {'question': 'How does QLoRA improve computational efficiency?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how QLoRA improves computational efficiency. It breaks down the concept into four key mechanisms, explaining each one in a way that is easy to understand. \\n\\nThe submission is insightful as it not only explains what QLoRA does, but also how it achieves its goal. It provides a comprehensive understanding of the topic.\\n\\nThe submission is appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5850d549-d7ef-4235-bb7d-a3e5e63cf413'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is detailed and explains how QLoRA improves computational efficiency. The student mentions that QLoRA reduces the number of parameters that need to be adjusted, which aligns with the context provided. The student also explains other ways QLoRA improves computational efficiency, such as through quantization, efficient memory usage, and speed improvements. All of these points are factually accurate and relevant to the question. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('734dc2ec-5405-4275-b37e-cf00b346979b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('76490a7d-8eb7-47e8-bdda-f326fc4ae22d'), target_run_id=None)],\n",
              "   'execution_time': 5.329876,\n",
              "   'run_id': '21803de2-8966-482e-aa53-37134f0ea9e3',\n",
              "   'output': \"QLoRA (Quantized Low-Rank Adaptation) is a technique designed to improve the computational efficiency of training and fine-tuning large language models. It achieves this through several key mechanisms:\\n\\n1. **Quantization**: QLoRA applies quantization to the model's weights, which reduces the precision of the weights from 32-bit floating-point numbers to lower precision formats (e.g., 8-bit integers). This significantly reduces the memory footprint and computational requirements for both storage and processing.\\n\\n2. **Low-Rank Adaptation**: Instead of updating all the parameters of the model during fine-tuning, QLoRA updates only a low-rank approximation of the weight matrices. This reduces the number of parameters that need to be adjusted, leading to faster training times and lower memory usage.\\n\\n3. **Efficient Memory Usage**: By combining quantization with low-rank adaptation, QLoRA minimizes the amount of memory required to store and process the model. This allows for the training and fine-tuning of large models on hardware with limited memory resources, such as GPUs with smaller VRAM.\\n\\n4. **Speed Improvements**: The reduction in the number of parameters and the use of lower precision arithmetic lead to faster computations. This results in shorter training times and quicker iterations during the fine-tuning process.\\n\\nOverall, QLoRA improves computational efficiency by reducing the memory and computational requirements through quantization and low-rank adaptation, enabling the use of large language models on more modest hardware and speeding up the training process.\",\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####ðŸ—ï¸ Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is evaluating how helpful the response is to determine whether to end of continue. It will go through this loop up to 10 times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####ðŸ—ï¸ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "We are adding nodes to the graph. This defines the flow of states and actions in an agent's process. It includes nodes for calling a model, invoking tools, and passing through a dummy node. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "Set the entry point of the graph to establish which node to call first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "Add the conditional edge to the graph to determine whether to continue or end, including checking helpfulness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "# Add the conditional edge to the graph to define whether to continue or end \n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Check the helpfulness of the response\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "Add the edge to the graph to connect the action node and the agent node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "\n",
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "Compiling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "Defines the inputs for the agent and has the agent respond to the user and ask how helpful its response is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA stands for Low-Rank Adaptation. It is a technique used in machine learning, particularly in the context of fine-tuning large pre-trained models. The main idea behind LoRA is to adapt a pre-trained model to a new task by introducing low-rank matrices into the model's architecture. This allows for efficient fine-tuning with fewer parameters, which can be particularly useful when dealing with large models that are computationally expensive to train.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher and expert in the field of machine learning and artificial intelligence. He is known for his work on efficient training and inference of large neural networks, particularly in the context of natural language processing (NLP). Dettmers has contributed to the development of techniques that make it feasible to train large models on consumer-grade hardware, such as GPUs. His research often focuses on optimizing the computational aspects of machine learning to make advanced models more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism used in neural networks, particularly in the context of sequence-to-sequence models like those used in natural language processing (NLP). The attention mechanism allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This is particularly useful in tasks like machine translation, where the model needs to align words in the source language with words in the target language.\n",
            "\n",
            "The most well-known form of attention is the \"self-attention\" mechanism used in Transformer models. In self-attention, each word in the input sequence is compared with every other word to determine which words are most relevant to the current word being processed. This allows the model to capture long-range dependencies and relationships between words, leading to better performance on a variety of NLP tasks.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP). It involves crafting specific inputs (prompts) to guide the behavior and responses of AI models, particularly large language models like GPT-3, GPT-4, and others. The goal is to elicit desired outputs from the model by carefully designing the input text.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Input Design**: Creating prompts that are clear, concise, and tailored to the specific task or question at hand.\n",
            "2. **Iterative Refinement**: Continuously refining prompts based on the responses received to improve the quality and relevance of the output.\n",
            "3. **Contextual Awareness**: Ensuring that the prompt provides enough context for the model to understand and generate appropriate responses.\n",
            "4. **Task-Specific Prompts**: Designing prompts that are specific to the task, whether it's text generation, question answering, summarization, translation, etc.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts highlighted the importance of crafting effective prompts to achieve desired outcomes. Researchers and practitioners quickly realized that the quality of the input prompt could significantly influence the quality of the model's output.\n",
            "\n",
            "### Timeline:\n",
            "- **Pre-2020**: Early NLP models and AI systems required more structured and explicit instructions. Prompt engineering was not a widely recognized practice.\n",
            "- **June 2020**: OpenAI released GPT-3, a powerful language model capable of generating coherent and contextually relevant text based on prompts. This release brought attention to the importance of prompt design.\n",
            "- **Post-2020**: The concept of prompt engineering gained traction as more researchers and developers began experimenting with large language models. The practice became a critical skill for effectively utilizing these models in various applications.\n",
            "\n",
            "In summary, prompt engineering is a technique used to guide AI models by designing specific inputs, and it gained significant attention with the release of GPT-3 in 2020.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both retrieval and generation to handle tasks that require access to a large amount of knowledge, such as open-domain question answering, dialogue systems, and other knowledge-intensive tasks.\n",
            "\n",
            "The RAG model typically consists of two main components:\n",
            "1. **Retriever**: This component retrieves relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates the final response or output by conditioning on both the input query and the retrieved documents.\n",
            "\n",
            "By combining these two components, RAG can generate more accurate and contextually relevant responses compared to models that rely solely on generative techniques.\n",
            "\n",
            "If you need more detailed information or the latest research on RAG, I can look up recent papers or articles for you.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Aspects of Fine-Tuning:\n",
            "1. **Pre-trained Model**: A model that has already been trained on a large and diverse dataset.\n",
            "2. **Target Task**: The specific task or dataset for which the model needs to be adapted.\n",
            "3. **Transfer Learning**: The underlying principle where knowledge from one domain (the pre-trained model) is transferred to another domain (the target task).\n",
            "\n",
            "### Steps in Fine-Tuning:\n",
            "1. **Select a Pre-trained Model**: Choose a model that has been trained on a large dataset.\n",
            "2. **Replace the Final Layer(s)**: Modify the final layer(s) of the model to match the number of classes or outputs of the target task.\n",
            "3. **Train on New Data**: Train the modified model on the new, smaller dataset, often with a lower learning rate to avoid overfitting.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the availability of large pre-trained models. Some key milestones include:\n",
            "\n",
            "- **2012**: The success of AlexNet in the ImageNet competition demonstrated the power of deep learning and pre-trained models.\n",
            "- **2014**: The introduction of transfer learning techniques in natural language processing (NLP) with models like Word2Vec and GloVe.\n",
            "- **2018**: The release of BERT (Bidirectional Encoder Representations from Transformers) by Google, which popularized fine-tuning in NLP. BERT was pre-trained on a large corpus and then fine-tuned for various NLP tasks, setting new benchmarks in the field.\n",
            "\n",
            "Fine-tuning has since become a standard practice in both computer vision and NLP, enabling the development of highly specialized models with relatively small amounts of data.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents, or Large Language Model-based agents, are systems that leverage large language models (LLMs) to perform a variety of tasks that involve understanding and generating human language. These tasks can include answering questions, generating text, translating languages, summarizing documents, and even engaging in conversations. LLMs are typically trained on vast amounts of text data and use deep learning techniques to understand and generate human-like text.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language input.\n",
            "2. **Natural Language Generation (NLG):** They can generate coherent and contextually appropriate text.\n",
            "3. **Context Awareness:** They can maintain context over a conversation or a series of interactions.\n",
            "4. **Versatility:** They can be applied to a wide range of applications, from chatbots to content creation to code generation.\n",
            "\n",
            "### Breakthrough and Evolution:\n",
            "The concept of LLM-based agents has been around for a while, but significant breakthroughs occurred with the development of models like OpenAI's GPT (Generative Pre-trained Transformer) series.\n",
            "\n",
            "1. **GPT-2 (2019):** This model demonstrated the potential of LLMs to generate coherent and contextually relevant text, sparking significant interest in the field.\n",
            "2. **GPT-3 (2020):** With 175 billion parameters, GPT-3 showcased even more impressive capabilities, including few-shot learning, where the model could perform tasks with very few examples.\n",
            "3. **Subsequent Developments:** Since GPT-3, there have been numerous advancements and iterations in LLMs, including models from other organizations like Google's BERT, T5, and more recently, models like GPT-4.\n",
            "\n",
            "### Applications:\n",
            "- **Customer Support:** Automated chatbots and virtual assistants.\n",
            "- **Content Creation:** Writing articles, generating marketing copy, and creative writing.\n",
            "- **Programming:** Code generation and debugging assistance.\n",
            "- **Education:** Personalized tutoring and educational content generation.\n",
            "- **Healthcare:** Assisting in medical documentation and patient interaction.\n",
            "\n",
            "The field continues to evolve rapidly, with ongoing research and development aimed at improving the capabilities, efficiency, and ethical considerations of LLM-based agents.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
